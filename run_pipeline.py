from datetime import datetime
from collections import defaultdict
from typing import List

from config.database import db
from nlp_engine.sentiment import analyze_sentiment
from nlp_engine.topic_model import run_topic_modeling
from nlp_engine.trend_analysis import analyze_trends
from nlp_engine.scoring import compute_opportunity_scores


POSTS_COLLECTION = db["posts"]


def load_preprocessed_posts(limit: int = 500) -> List[dict]:
    cursor = POSTS_COLLECTION.find(
        {
            "preprocessed": True,
            "is_candidate": True,
            "processed_text": {"$exists": True, "$ne": ""}
        }
    ).limit(limit)

    posts = list(cursor)
    print(f"‚úÖ Loaded {len(posts)} preprocessed posts")
    return posts


def main():
    print("\nüöÄ Starting NLP Opportunity Pipeline\n")

    # 1Ô∏è‚É£ Load data
    posts = load_preprocessed_posts()

    # üîç SHOW SAMPLE PREPROCESSED DATA
    print("\nüîç SAMPLE PREPROCESSED POSTS\n")
    for i, p in enumerate(posts[:5]):  # show first 5 only
        print(f"Post {i+1}")
        print("Original title:", p.get("title"))
        print("Original text:", p.get("selftext", "")[:200])
        print("Processed text:", p.get("processed_text", "")[:200])
        print("-" * 60)

    texts = [p["processed_text"] for p in posts]
    timestamps = [p.get("created_utc", datetime.utcnow()) for p in posts]

    # 2Ô∏è‚É£ Sentiment Analysis
    print("üîπ Running sentiment analysis...")
    sentiments = [analyze_sentiment(t) for t in texts]

    # 3Ô∏è‚É£ Topic Modeling
    print("üîπ Running topic modeling (BERTopic)...")
    topics, topic_keywords = run_topic_modeling(texts)

    # 4Ô∏è‚É£ Trend Analysis
    print("üîπ Analyzing topic trends...")
    trend_scores = analyze_trends(topics, timestamps)

    # 5Ô∏è‚É£ Aggregate per-topic stats
    topic_agg = defaultdict(lambda: {
        "count": 0,
        "sentiment_sum": 0.0
    })

    for topic, sent in zip(topics, sentiments):
        if topic == -1:
            continue
        topic_agg[topic]["count"] += 1
        topic_agg[topic]["sentiment_sum"] += sent["compound"]

    # 6Ô∏è‚É£ Build topic_stats for scoring
    topic_stats = {}

    for topic, stats in topic_agg.items():
        topic_stats[topic] = {
            "demand": stats["count"],
            "sentiment": stats["sentiment_sum"] / stats["count"],
            "trend": trend_scores.get(topic, 0.0),
            "competition": 0.5  # placeholder
        }

    # 7Ô∏è‚É£ Compute Opportunity Scores
    print("üîπ Computing opportunity scores...")
    scores = compute_opportunity_scores(topic_stats)

    # 8Ô∏è‚É£ Prepare ranked output
    opportunities = []

    for topic, score in scores.items():
        opportunities.append({
            "topic": topic,
            "score": score,
            "volume": topic_stats[topic]["demand"],
            "trend": topic_stats[topic]["trend"],
            "keywords": topic_keywords.get(topic, [])
        })

    opportunities.sort(key=lambda x: x["score"], reverse=True)

    # üî• OUTPUT
    print("\nüéØ TOP OPPORTUNITIES\n")
    for opp in opportunities[:5]:
        print(f"Topic ID: {opp['topic']}")
        print(f"Score: {opp['score']}")
        print(f"Volume: {opp['volume']}")
        print(f"Trend: {opp['trend']:.2f}")
        print(f"Keywords: {opp['keywords']}")
        print("-" * 40)

    print("\n‚úÖ Pipeline completed successfully!")


if __name__ == "__main__":
    main()
